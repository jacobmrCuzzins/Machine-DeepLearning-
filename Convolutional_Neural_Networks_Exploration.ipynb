{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobmrCuzzins/Machine-DeepLearning-/blob/main/Convolutional_Neural_Networks_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDcdZVVIswy"
      },
      "source": [
        "# Classifying Images Using CNN\n",
        "This notebook is to explore the concepts with respect to training and evaluating classifiers such as the Convolutional Neural Networks (CNNs).\n",
        "\n",
        "[https://keras.io/](https://keras.io/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC0Dh4dwQ0Iq"
      },
      "source": [
        "## Accessing Data from Google Drive\n",
        "The dataset for this assignment is the CIFAR-10 dataset that can be found here:\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "The CIFAR-10 and CIFAR-100 are well studied, yet challenging image recognition dataset. The CIFAR-10 has up to 10 classes to classify and contains 60,000 32x32 images. You should read the description of the dataset and download the dataset for Python, that is\n",
        "\n",
        "CIFAR-10 python version: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "\n",
        "Once downloaded you need to then extract and upload the `cifar-10-batches-py` directory your Google Drive so that you can access it from within your Google Colab.\n",
        "\n",
        "You can mount the Google Drive from the menu on the left or uncomment use the code below mount the drive.  See here for documentation on file access in Colab:\n",
        "\n",
        "[External data: Local Files, Drive, Sheets, and Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqR-k4UDDltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa5690e-3c8f-48e9-e29f-82eeb1345170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s7DsaxDI2im"
      },
      "source": [
        "## Functions to work with CIFAR\n",
        "\n",
        "The functions below help with access to the CIFAR-10 data the you have downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUSVhQr5KDv1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def load_CIFAR_batch(filename, flatten=True, categorical=True):\n",
        "    \"\"\" load single batch of cifar \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding='bytes')\n",
        "        X = datadict[b'data']\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
        "        if (flatten):\n",
        "          X = X.reshape(10000, 3072)\n",
        "        X = X.astype('float32')\n",
        "        X /= 255\n",
        "\n",
        "        y = datadict[b'labels']\n",
        "        y = np.array(y)\n",
        "        if (categorical):\n",
        "          y = pd.get_dummies(y).values\n",
        "\n",
        "        return X, y\n",
        "\n",
        "def load_CIFAR_meta(filename):\n",
        "  with open(filename,'rb') as f:\n",
        "    metadict = pickle.load(f, encoding='bytes')\n",
        "\n",
        "    class_labels = [ val.decode() for val in metadict.get(b'label_names') ]\n",
        "    return class_labels\n",
        "\n",
        "def get_image(X, index, nchans=3, size=32):\n",
        "  xi = X[index,:]\n",
        "  img = xi.reshape(32, 32, 3)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axqceDnNJVss"
      },
      "source": [
        "## Load the CIFAR data\n",
        "\n",
        "The CIFAR data has 5 batches of data and 1 test data set. Each batch is labelled\n",
        "- `data_batch_1`\n",
        "- `data_batch_2`\n",
        "- `data_batch_3`\n",
        "- `data_batch_4`\n",
        "- `data_batch_5`\n",
        "\n",
        "and a test set labelled\n",
        "- `test_batch`\n",
        "\n",
        "each batch has 10,000 images, so 50,000 training and 10,000 test images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2X0_Oz_SkHi"
      },
      "source": [
        "Below is example of loading the first batch of training data labelled as `data_batch_1`.  You will need to update the path to match where you have stored your cifar-10 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SzM-lMGRVDr"
      },
      "source": [
        "LOADING ONLY ONE BATCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sFJMu_JNZaw",
        "outputId": "98b8e1e6-23e9-4722-9f7a-b870c7f654cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 10000 instances of images\n"
          ]
        }
      ],
      "source": [
        "X, y = load_CIFAR_batch('/content/drive/My Drive/cifar-10-batches-py/data_batch_1', flatten = False)\n",
        "\n",
        "# split into train and test\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, stratify=y)\n",
        "\n",
        "# Create 1d version for testing accuracy\n",
        "ytrain_1d = np.argmax(ytrain, axis=1)\n",
        "ytest_1d = np.argmax(ytest, axis=1)\n",
        "\n",
        "print('We have {} instances of images'.format(y.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_c-D8WgRZQ9"
      },
      "source": [
        "TO LOAD THE FULL BATCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SudeLpyHCd-X",
        "outputId": "54bf25ce-1856-4d04-ca0b-9c84805e28f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 50000 instances of images\n"
          ]
        }
      ],
      "source": [
        "flatten = False\n",
        "categorical = True\n",
        "\n",
        "# load only the first batch\n",
        "#X1, y1 = load_CIFAR_batch('/content/drive/My Drive/cifar-10-batches-py/data_batch_1',flatten=flatten,categorical=categorical)\n",
        "\n",
        "# load only the second batch\n",
        "#X2, y2 = load_CIFAR_batch('/content/drive/My Drive/COMP2712/data/cifar-10-batches-py/data_batch_2',flatten=flatten,categorical=categorical)\n",
        "\n",
        "# Load the first batch from CIFAR-10\n",
        "X, y = load_CIFAR_batch('/content/drive/My Drive/cifar-10-batches-py/data_batch_1',flatten= False ,categorical=categorical)\n",
        "\n",
        "# iterate over 2 to 5\n",
        "for bi in range(2,6):\n",
        "  # load the next data set 'bi'\n",
        "  X2, y2 = load_CIFAR_batch('/content/drive/My Drive/cifar-10-batches-py/data_batch_{}'.format(bi),flatten=flatten,categorical=categorical)\n",
        "\n",
        "  # concatenate/stack the dataest together\n",
        "  X = np.vstack([X, X2])\n",
        "  y = np.vstack([y, y2])\n",
        "\n",
        " #Load the test data into test variables\n",
        "Xtest, ytest = load_CIFAR_batch('/content/drive/My Drive/cifar-10-batches-py/test_batch', flatten=False)\n",
        "\n",
        "# Create 1d version for testing accuracy\n",
        "ytrain_1d = np.argmax(y, axis=1)\n",
        "ytest_1d = np.argmax(ytest, axis=1)\n",
        "\n",
        "print('We have {} instances of images'.format(y.shape[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMUnAt_YKPjp"
      },
      "source": [
        "The number of instances/examples for all the different classes.  There are 10 different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-uX5AZmQG9N",
        "outputId": "fada0470-7087-43a5-f94f-c0c0d617de31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "[np.sum(np.argmax(y, axis=1) == i) for i in range(0,10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_HIgldKlsd"
      },
      "source": [
        "The labels for the classes are stored in the `batches.meta` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HKjGtxStmcz",
        "outputId": "9648274d-1bdd-4e83-db5e-2f5406054655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "class_labels = load_CIFAR_meta('/content/drive/My Drive/cifar-10-batches-py/batches.meta')\n",
        "print(class_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-jVltLJGonf"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZUEy4WrReuh"
      },
      "source": [
        "**BUILD THE CNN MODEL FROM ASSIGNMENT SHEET**\n",
        "\n",
        "This is the CNN Model built from the given structure in the assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O-WATT2RNkE"
      },
      "outputs": [],
      "source": [
        "X_flatten = X.reshape((50000, -1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQHtAN7yMeQu",
        "outputId": "fad75e65-9c65-4474-ae2f-c8b86dba539e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuMfYH79Tc8D",
        "outputId": "8740779f-6856-4503-97ae-9cfd529f7260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 30, 30, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 15, 15, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 13, 13, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 4, 4, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               262272    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 357706 (1.36 MB)\n",
            "Trainable params: 357258 (1.36 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "\n",
        "\n",
        "model_cnn = Sequential()\n",
        "\n",
        "# Add the first convolutional layer\n",
        "model_cnn.add(Conv2D(32, kernel_size=(3, 3), input_shape=(32, 32, 3)))\n",
        "\n",
        "model_cnn.add(BatchNormalization())  # Add Batch Normalization\n",
        "model_cnn.add(Activation('relu'))  # Add activation function\n",
        "\n",
        "\n",
        "# Add a max pooling layer\n",
        "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a second convolutional layer\n",
        "model_cnn.add(Conv2D(64, kernel_size=(3, 3)))\n",
        "model_cnn.add(BatchNormalization())  # Add Batch Normalization\n",
        "model_cnn.add(Activation('relu'))  # Add activation function\n",
        "\n",
        "# Add another max pooling layer\n",
        "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a third convolutional layer\n",
        "model_cnn.add(Conv2D(128, kernel_size=(3, 3)))\n",
        "model_cnn.add(BatchNormalization())  # Add Batch Normalization\n",
        "model_cnn.add(Activation('relu'))  # Add activation function\n",
        "\n",
        "# Flatten the output for the dense layers\n",
        "model_cnn.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with 128 units\n",
        "model_cnn.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add a dropout layer for regularization\n",
        "model_cnn.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer for classification\n",
        "model_cnn.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model_cnn.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements made in this model:\n",
        "\n",
        "Added an additional convolutional layer to capture more complex features.\n",
        "Added an additional max pooling layer to reduce spatial dimensions.\n",
        "Added a dropout layer with a rate of 0.5 for regularization.\n",
        "Increased the number of filters in the convolutional layers for more feature extraction capacity.\n",
        "Please note that while these changes can potentially improve the performance of the model, it's important to experiment and fine-tune the architecture and hyperparameters based on your specific dataset and task. Additionally, monitor the training process and evaluate the model's performance on a separate validation set."
      ],
      "metadata": {
        "id": "PXRNE0ex7B7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn evaluation packages\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
      ],
      "metadata": {
        "id": "AHLOku8nsQZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "oMMUY1KyCrCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5x Stratefied K-fold for evaluation of model**"
      ],
      "metadata": {
        "id": "-90n7VHq_2Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified K-Fold for evaluation of generalisation performance\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "k = 5 # 10 is the gold standard\n",
        "kf = StratifiedKFold(n_splits=k)\n",
        "foldi = 1\n",
        "ac = [] # accuracy_score\n",
        "cr = [] # classification_report\n",
        "cm = [] # confusion_matrix\n",
        "y_test_max_all = []\n",
        "y_pred_max_all = []\n",
        "\n",
        "activation = 'relu'\n",
        "no_epochs = 15\n",
        "\n",
        "\n",
        "for train_index, test_index in kf.split(X_flatten,ytrain_1d):\n",
        "\n",
        "  print('Training with {0} for {1} for fold {2} or {3}'.format(activation, no_epochs, foldi, k))\n",
        "  history = model_cnn.fit(X, y, epochs=no_epochs, verbose=1, validation_data=(Xtest, ytest),  # Add validation data\n",
        "              callbacks=[early_stopping])  # Add callbacks)\n",
        "\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "\n",
        "  loss_final = np.sqrt(float(hist['loss'].tail(1)))\n",
        "  print()\n",
        "  print('Final Loss on training set: {}'.format(round(loss_final, 3)))\n",
        "\n",
        "  # evaluate the model\n",
        "  y_pred_real = model_cnn.predict(Xtest)\n",
        "  y_pred_max = np.argmax(y_pred_real, axis=-1).astype(int)\n",
        "  y_test_max = ytest.argmax(axis=1).astype(int)\n",
        "\n",
        "  y_pred_max_all.extend(y_pred_max)\n",
        "  y_test_max_all.extend(y_test_max)\n",
        "\n",
        "  ac.append(accuracy_score(y_test_max,y_pred_max))\n",
        "  print('accuracy is {:.2f}%'.format(ac[-1]*100)) # Print accuracy score\n",
        "  print()\n",
        "  cr.append(classification_report(y_test_max,y_pred_max)) # Print summary report\n",
        "  cm.append(confusion_matrix(y_test_max, y_pred_max))\n",
        "\n",
        "  foldi = foldi + 1\n",
        "\n",
        "# Calculate the average accuracy and standard deviation\n",
        "average_accuracy = np.mean(ac) * 100\n",
        "std_dev_accuracy = np.std(ac) * 100\n",
        "\n",
        "# Print the results\n",
        "print(f'Average Accuracy: {average_accuracy:.2f}%')\n",
        "print(f'Standard Deviation of Accuracy: {std_dev_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "print('k-fold complete!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pduoUdsTn6EQ",
        "outputId": "414481f9-b4b7-406c-8964-85e6ef2654b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with relu for 15 for fold 1 or 5\n",
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.2285 - accuracy: 0.9201 - val_loss: 0.9953 - val_accuracy: 0.7650\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2210 - accuracy: 0.9224 - val_loss: 1.1894 - val_accuracy: 0.7478\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.2129 - accuracy: 0.9265 - val_loss: 1.1285 - val_accuracy: 0.7529\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2105 - accuracy: 0.9263 - val_loss: 1.1072 - val_accuracy: 0.7505\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2055 - accuracy: 0.9281 - val_loss: 1.0418 - val_accuracy: 0.7619\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1985 - accuracy: 0.9297 - val_loss: 1.1932 - val_accuracy: 0.7508\n",
            "\n",
            "Final Loss on training set: 0.446\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "accuracy is 76.50%\n",
            "\n",
            "Training with relu for 15 for fold 2 or 5\n",
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2216 - accuracy: 0.9239 - val_loss: 1.0571 - val_accuracy: 0.7488\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2170 - accuracy: 0.9243 - val_loss: 1.0392 - val_accuracy: 0.7636\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2101 - accuracy: 0.9267 - val_loss: 1.1095 - val_accuracy: 0.7548\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2015 - accuracy: 0.9296 - val_loss: 1.4997 - val_accuracy: 0.7144\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1988 - accuracy: 0.9305 - val_loss: 1.2004 - val_accuracy: 0.7414\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1903 - accuracy: 0.9331 - val_loss: 1.1233 - val_accuracy: 0.7632\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1886 - accuracy: 0.9346 - val_loss: 1.1490 - val_accuracy: 0.7573\n",
            "\n",
            "Final Loss on training set: 0.434\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "accuracy is 76.36%\n",
            "\n",
            "Training with relu for 15 for fold 3 or 5\n",
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2076 - accuracy: 0.9279 - val_loss: 1.1448 - val_accuracy: 0.7563\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2061 - accuracy: 0.9285 - val_loss: 1.1116 - val_accuracy: 0.7609\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1966 - accuracy: 0.9319 - val_loss: 1.2741 - val_accuracy: 0.7390\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1894 - accuracy: 0.9340 - val_loss: 1.1949 - val_accuracy: 0.7629\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1912 - accuracy: 0.9330 - val_loss: 1.1064 - val_accuracy: 0.7630\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1802 - accuracy: 0.9371 - val_loss: 1.2373 - val_accuracy: 0.7430\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1787 - accuracy: 0.9373 - val_loss: 1.1525 - val_accuracy: 0.7653\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1786 - accuracy: 0.9383 - val_loss: 1.2180 - val_accuracy: 0.7639\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1690 - accuracy: 0.9410 - val_loss: 1.1709 - val_accuracy: 0.7639\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1703 - accuracy: 0.9408 - val_loss: 1.2879 - val_accuracy: 0.7469\n",
            "\n",
            "Final Loss on training set: 0.413\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "accuracy is 76.30%\n",
            "\n",
            "Training with relu for 15 for fold 4 or 5\n",
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1842 - accuracy: 0.9351 - val_loss: 1.2053 - val_accuracy: 0.7549\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1829 - accuracy: 0.9372 - val_loss: 1.4679 - val_accuracy: 0.7292\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1784 - accuracy: 0.9387 - val_loss: 1.3183 - val_accuracy: 0.7530\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1752 - accuracy: 0.9394 - val_loss: 1.2516 - val_accuracy: 0.7499\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1710 - accuracy: 0.9416 - val_loss: 1.2378 - val_accuracy: 0.7601\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1629 - accuracy: 0.9449 - val_loss: 1.1902 - val_accuracy: 0.7650\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1646 - accuracy: 0.9434 - val_loss: 1.2781 - val_accuracy: 0.7526\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1609 - accuracy: 0.9436 - val_loss: 1.3480 - val_accuracy: 0.7324\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1581 - accuracy: 0.9469 - val_loss: 1.2686 - val_accuracy: 0.7641\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1535 - accuracy: 0.9482 - val_loss: 1.6522 - val_accuracy: 0.7170\n",
            "Epoch 11/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1523 - accuracy: 0.9472 - val_loss: 1.2372 - val_accuracy: 0.7675\n",
            "\n",
            "Final Loss on training set: 0.39\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "accuracy is 76.50%\n",
            "\n",
            "Training with relu for 15 for fold 5 or 5\n",
            "Epoch 1/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1722 - accuracy: 0.9415 - val_loss: 1.3201 - val_accuracy: 0.7457\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1618 - accuracy: 0.9437 - val_loss: 1.4685 - val_accuracy: 0.7272\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1599 - accuracy: 0.9445 - val_loss: 1.2504 - val_accuracy: 0.7663\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1522 - accuracy: 0.9477 - val_loss: 1.3928 - val_accuracy: 0.7515\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1538 - accuracy: 0.9469 - val_loss: 1.1809 - val_accuracy: 0.7712\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1467 - accuracy: 0.9489 - val_loss: 1.4128 - val_accuracy: 0.7416\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1511 - accuracy: 0.9486 - val_loss: 1.4664 - val_accuracy: 0.7431\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1416 - accuracy: 0.9513 - val_loss: 1.8604 - val_accuracy: 0.7006\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1381 - accuracy: 0.9540 - val_loss: 1.3424 - val_accuracy: 0.7580\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.1406 - accuracy: 0.9518 - val_loss: 1.3342 - val_accuracy: 0.7624\n",
            "\n",
            "Final Loss on training set: 0.375\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "accuracy is 77.12%\n",
            "\n",
            "Average Accuracy: 76.56%\n",
            "Standard Deviation of Accuracy: 0.29%\n",
            "k-fold complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}